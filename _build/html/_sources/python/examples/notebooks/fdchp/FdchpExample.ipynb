{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfa50afa-1294-4f37-9f9b-8b85e47db087",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# FDCHP Data Processing\n",
    "\n",
    "This Notebook demonstrates how to process FDCHP data to calculate fluxes using the tools in the `ooi-data-explorations.python.examples.fdchp` library. The FDCHP instrument is on for 20 minutes out of each hour. While the instrument is recording, it includes 10 Hz 3-axis sonic wind data, as well as buoy motion data. Here, we calculate hourly motion-corrected momentum flux from these 20-minute data segments. \n",
    "\n",
    "To do the calculations, we need to set up a python environment that contains the necessary dependencies.\n",
    "\n",
    "Alongside the base repository containing this Notebook (the location of the `ooi-data-explorations` repository, clone the `mi-instrument` repo \n",
    "\n",
    "```\n",
    "git clone https://github.com/joffreyp/mi-instrument.git\n",
    "```\n",
    "\n",
    "Create a new python virtual environment:\n",
    "\n",
    "```\n",
    "cd ~\n",
    "mkdir venv\n",
    "python -m venv ~/venv\n",
    "source venv/bin/activate\n",
    "```\n",
    "\n",
    "Install mi-instrument requirements:\n",
    "\n",
    "```\n",
    "pip install -r mi-instrument/requirements_py3.txt\n",
    "```\n",
    "\n",
    "Install `ooi-data-explorations` and `mi-instruments`: \n",
    "\n",
    "```\n",
    "pip install -e ooi-data-explorations/python/\n",
    "pip install -e mi-instrument/\n",
    "```\n",
    "\n",
    "Prepare environment for use by Jupyter Notebook kernel:\n",
    "\n",
    "```\n",
    "pip install ipykernel\n",
    "pip install ipympl\n",
    "python -m ipykernel install --user --name=fdchp\n",
    "vi ~/.local/share/jupyter/kernels/fdchp/kernel.json\n",
    "```\n",
    "\n",
    "Check that the first line of the `argv` array in `kernel.json` points to the python from the new virtual environment: `\"/home/jovyan/venv/bin/python\",`\n",
    "\n",
    "Now, select the `Kernel` menu, click `Change kernel...` and select the new `fdchp` environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767717cd-1c45-48d7-ac98-1e1381247ed1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44d9795c-13e9-48ad-bac7-363a4646ee7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "\n",
    "from examples.fdchp import fdchp_utils\n",
    "from examples.fdchp.fdchp_utils import particles_to_pandas, read_file, read_file_to_pandas, convert_data_to_nwu\n",
    "from examples.fdchp.process_fdchp import process_fdchp, process_fdchp_xarray\n",
    "from examples.fdchp.plot_fdchp import plot_x_velocity_vs_wind_speed, plot_wave_height_vs_wind_speed\n",
    "\n",
    "from ooi_data_explorations.common import kdata_collect_from_file_list\n",
    "from ooi_data_explorations.common import load_kdata, m2m_request, m2m_collect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12dca8a-d67b-425a-83d0-dfa861b09554",
   "metadata": {},
   "source": [
    "## Get reference designator, stream, method, start and end dates if specified\n",
    "Some links provide reference designator, stream, method, start and end date information. These end up stored as environment variables. Here, we extract them and set them as python variables in this notebook. \n",
    "\n",
    "If these parameters were not set, they default to the recovered data for the `GI01SUMO-SBD12-08-FDCHPA000` instrument for the month of September 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3030d31-8e7c-48c8-b938-e718b5510bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "refdes: GI01SUMO-SBD12-08-FDCHPA000, stream: fdchp_a_instrument_recovered, method: recovered_inst, start_time: 2023-09-01 00:00:00+00:00, end_time: 2023-09-30 23:59:59.999000+00:00\n"
     ]
    }
   ],
   "source": [
    "refdes = os.environ.get('ooiparams_refdes', 'GI01SUMO-SBD12-08-FDCHPA000')\n",
    "stream = os.environ.get('ooiparams_stream', 'fdchp_a_instrument_recovered')\n",
    "method = os.environ.get('ooiparams_method', 'recovered_inst');\n",
    "start_time = datetime.fromisoformat(os.environ.get('ooiparams_startTime', '2023-09-01T00:00:00.000Z')).astimezone(timezone.utc)\n",
    "end_time = datetime.fromisoformat(os.environ.get('ooiparams_endTime', '2023-09-30T23:59:59.999Z')).astimezone(timezone.utc)\n",
    "print(\"refdes: {}, stream: {}, method: {}, start_time: {}, end_time: {}\".format(refdes, stream, method, start_time, end_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031f6649-ca2b-4034-8558-f8a6f4f5c34b",
   "metadata": {},
   "source": [
    "### Defining functions for filtering NetCDF files based on data dates contained in the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c443725a-2b06-47c5-940a-4b4596fa43cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_dates(filename, stream):\n",
    "    # deployment0010_GI01SUMO-SBD12-08-FDCHPA000-recovered_inst-fdchp_a_instrument_recovered_20231222T210200.048000-20240107T202200.041000.nc\n",
    "    dates = filename.split('_')[-1].split('.nc')[0].split('-')\n",
    "    return datetime.fromisoformat(dates[0]).astimezone(timezone.utc), datetime.fromisoformat(dates[1]).astimezone(timezone.utc)\n",
    "\n",
    "def limit_files(file_list, start_date, end_date):\n",
    "    files = []\n",
    "    for file in file_list:\n",
    "        start, end = get_file_dates(file, stream)\n",
    "        if start <= end_date and end >= start_date:\n",
    "            files.append(file)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515732e4-204f-4cf7-8343-2a241a62fa20",
   "metadata": {},
   "source": [
    "### Selecting all files for this instrument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f0ad623-9a4f-45ef-bf3e-c947af3d86fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "refdes_parts = refdes.split('-')\n",
    "site = refdes_parts[0]\n",
    "node = refdes_parts[1]\n",
    "sensor = '-'.join(refdes_parts[2:])\n",
    "\n",
    "dataset_id = '-'.join([site, node, sensor, method, stream])\n",
    "kdata = os.path.abspath(os.path.join(os.path.expanduser('~'), 'ooi/kdata-backups/exclude_folder_removal'))\n",
    "kdata = os.path.abspath(os.path.join(kdata, dataset_id))\n",
    "\n",
    "# Get all netcdf files for site, node, sensor, method, stream\n",
    "files = files = glob(kdata + '/' + 'deployment*' + dataset_id + '*.nc')\n",
    "# print(len(files))\n",
    "# filter files based on start and end times\n",
    "file_list = limit_files(files, start_time, end_time)\n",
    "# for file in file_list:\n",
    "#     print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02237a8-514e-412e-8bcc-c7c8c5bd7d03",
   "metadata": {},
   "source": [
    "### Open NetCDF files as xarray datasets, setting `time` as the primary dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25190fb3-38dc-4116-a384-09703f45f06a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m xarrays = [ xr.open_dataset(file).swap_dims({\u001b[33m\"\u001b[39m\u001b[33mobs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mtime\u001b[39m\u001b[33m\"\u001b[39m}) \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfile_list\u001b[49m ]\n",
      "\u001b[31mNameError\u001b[39m: name 'file_list' is not defined"
     ]
    }
   ],
   "source": [
    "xarrays = [ xr.open_dataset(file).swap_dims({\"obs\": \"time\"}) for file in file_list ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8d50f5-1262-448c-ae59-1d30496889c9",
   "metadata": {},
   "source": [
    "### Determine which variables can be dropped before merging datasets, and trim variables from datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd0e92e-cf1e-4151-ade3-28f32b5eeb31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vars_to_drop = [var for var in xarrays[0].variables if var.startswith('fdchp_a')]\n",
    "vars_to_drop += ['obs', 'year', 'month', 'day', 'hour', 'minute', 'second', 'millisecond']\n",
    "trimmed_datasets = []\n",
    "for ds in xarrays:\n",
    "    trimmed_datasets.append(ds.drop_vars(vars_to_drop, errors=\"ignore\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be350646-f591-4ade-aacf-50c64f69fe02",
   "metadata": {},
   "source": [
    "### Combine trimmed datasets using the `time` dimension and subselect data between start and end times.\n",
    "Also, rotate dataset to NWU coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae9e677-b1a3-4aa2-bd92-632f248b528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = xr.concat(trimmed_datasets, dim='time')\n",
    "start_time_64 = pd.Timestamp(start_time).to_datetime64()\n",
    "end_time_64 = pd.Timestamp(end_time).to_datetime64()\n",
    "data = data.sel(time=slice(start_time_64, end_time_64))\n",
    "convert_data_to_nwu(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb4e0a7-3024-4009-9581-2f6f88a60aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below may work fine in the future, but given the complexity of the multiple time dimensions in the current recovered netcdfs xarray's broadcasting is overwhelmed and it thinks it needs hundreds of TiB of memory to do the dataset merge.\n",
    "# data = kdata_collect_from_file_list(file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c8d0e6-5697-43da-a6a2-c85c5ca37e3d",
   "metadata": {},
   "source": [
    "### Define xarray dataset processing to get motion-corrected fluxes from fdchp data\n",
    "Note that the function `process_fdchp_xarray` is in the `ooi-data-explorations/python/examples/fdchp/process_fdchp.py` module. It calls many functions in the `ooi-data-explorations/python/examples/fdchp/fdchp_utils.py` module. See those python modules for details on how the computation is performed, and to potentially modify the filtering as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16e9f8c-6214-41ab-9ca2-8f5a3347a72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_xarray(dataset, output_filepath='fluxes'):\n",
    "    # Set up some variables for FDCHP processing depending on sensor location\n",
    "    subsite = dataset.subsite\n",
    "    if subsite.startswith('PA'):\n",
    "        lat=40.1334        #Pioneer NES\n",
    "        instrument_rel_position=[-0.75, 0, -5]\n",
    "    elif subsite.startswith('IS'):\n",
    "        lat=59.9337        #Irminger Sea\n",
    "        instrument_rel_position=[-0.75, 0, -6] \n",
    "    else:\n",
    "        lat=44.6393        #Endurance\n",
    "        instrument_rel_position=[-0.5, -0.5, -5]\n",
    "    \n",
    "    output_filename = \"fluxes{}\"\n",
    "    errors = {}\n",
    "    U=[]\n",
    "    uw=[]\n",
    "    sigH=[]\n",
    "    times=[]\n",
    "    data = []\n",
    "    if not os.path.exists(output_filepath):\n",
    "        os.makedirs(output_filepath)\n",
    "\n",
    "    print(\"Start time: {}\".format(datetime.now()))\n",
    "    data_processed_accumulator = timedelta(0)\n",
    "    \n",
    "    start = datetime.now()\n",
    "    incr = 0\n",
    "    #*****************************************\n",
    "    # Compute flux data\n",
    "    #***************************************** \n",
    "    hourly_groups = dataset.resample(time=\"1h\", offset=-timedelta(minutes=10)) #offsetting by 10 minutes because a few datasets sometimes start before the hour, but typically end around 22 minutes into the hour.\n",
    "    tot_hours = len(hourly_groups)\n",
    "    for group_name, group_ds in hourly_groups:\n",
    "        try:\n",
    "            fluxes, Uearth, waveheight = process_fdchp_xarray(group_ds, lat, instrument_rel_position)\n",
    "        except Exception as e:\n",
    "            # Error processing data; probably too few datapoints\n",
    "            print(\"Error processing fdchp dataset: {}\".format(e))\n",
    "            errors[filename] = e\n",
    "            continue\n",
    "        finally:\n",
    "            data_processed_accumulator = data_processed_accumulator + (datetime.now()-start)\n",
    "            incr = incr + 1\n",
    "            if incr %24 == 0:\n",
    "                print(\"File processing finished for hour {} of {}. Process time: {}\".format(incr, tot_hours, data_processed_accumulator))\n",
    "        \n",
    "        uw = uw + [fluxes[0]]        # Fluxes: uw vw wT\n",
    "        U = U + [Uearth]              # Wind speed relative to earth  \n",
    "        sigH = sigH + [waveheight]    # Significant wave height\n",
    "        times = times + [group_ds['time'].mean().to_numpy()]\n",
    "    \n",
    "    uw = np.array(uw)\n",
    "    U = np.array(U)\n",
    "    sigH = np.array(sigH)\n",
    "    times = np.array(times)\n",
    "    \n",
    "    print(\"{} errors during processing.\".format(len(errors)))\n",
    "    print(\"End time: {}\".format(datetime.now()))\n",
    "    print(\"Processed {} hours of data in {}.\".format(tot_hours, datetime.now() - start))\n",
    "\n",
    "    return uw, U, sigH, times, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d832afb-c073-4e17-bd52-e35284ffb0f0",
   "metadata": {},
   "source": [
    "## Process data to calculate fluxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e25eb9-9b5b-42ce-a012-0db3e8fb6653",
   "metadata": {},
   "outputs": [],
   "source": [
    "uw, U, sigH, times, errors =  process_xarray(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a93a2d-1f7d-4544-aa16-2af25e8f217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_x_velocity_vs_wind_speed(U, uw)\n",
    "plot_wave_height_vs_wind_speed(U, sigH, x_max = 20.0, y_max=8.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e07f7fc-5237-4942-bc8f-857d9e9b5629",
   "metadata": {},
   "source": [
    "## Get Telemetered data from kdata directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04faf61d-799f-4843-849d-04cc23dae430",
   "metadata": {},
   "source": [
    "Find all deployments, assemble and join datasets. This presumes that the initial datasets were recovered of some kind. If not, modify the code below to pick a different method (replace the 'telemetered' parameter in `load_kdata` below).\n",
    "\n",
    "The telemetered datasets include the fluxes calculated onboard the FDCHP instrument with flux values telemetered each hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c94262-e551-4c68-87f2-be4d2ced6bca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ooi_data_explorations.common import load_kdata\n",
    "\n",
    "deployments = {}\n",
    "for file in file_list:\n",
    "    deployment = file.split('deployment')[-1][:4]\n",
    "    deployments['deployment'+deployment] = file\n",
    "\n",
    "print(\"Deployments: {}\".format(deployments.keys()))\n",
    "\n",
    "# load datasets\n",
    "telemetered_datasets = []\n",
    "for deployment in deployments.keys():\n",
    "    telemetered_datasets.append(load_kdata(site, node, sensor, 'telemetered', 'fdchp_a_dcl_instrument', tag= deployment + '*.nc'))\n",
    "\n",
    "# join datasets\n",
    "telemetered_dataset = xr.concat(telemetered_datasets, dim='time')\n",
    "\n",
    "telemetered_dataset = telemetered_dataset.sel(time=slice(start_time_64, end_time_64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46c9da1-9212-4a9a-a96e-7d04157ea196",
   "metadata": {},
   "source": [
    "## Plot calculated vs telemetered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dfa902-9ad1-475f-ae1c-03c972b36401",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.collections import PathCollection\n",
    "from matplotlib.legend_handler import HandlerPathCollection, HandlerLine2D\n",
    "%matplotlib inline\n",
    "# %matplotlib ipympl\n",
    "\n",
    "marker_size = 3\n",
    "legend_marker_size = 20\n",
    "tick_size = 14\n",
    "\n",
    "def update(handle, orig):\n",
    "    handle.update_from(orig)\n",
    "    handle.set_sizes([legend_marker_size])\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.plot(times, uw, marker='o',linestyle='solid',markersize=marker_size, color='green', alpha=0.5, label=\"Computed\")\n",
    "\n",
    "scatter = ax.plot(telemetered_dataset['time'], telemetered_dataset['uw_momentum_flux'], marker='o',linestyle='solid', color='orange', markersize=marker_size, alpha=0.5, label=\"Telemetered\")\n",
    "ax.set_xlabel('Time', fontsize=22)\n",
    "ax.set_xlim(np.min(times), np.max(times))\n",
    "ax.set_ylabel('uw Along-wind momentum flux', fontsize=22)\n",
    "ax.set_ylim(-1.0, 0.05)\n",
    "legend = ax.legend(loc=\"lower right\", fontsize=tick_size)\n",
    "\n",
    "plt.xticks(fontsize=tick_size)\n",
    "plt.yticks(fontsize=tick_size)\n",
    "plt.title(\"Along-Wind Momentum Flux Comparison\", fontsize=24)\n",
    "# plt.savefig('calculated_vs_telemetered_comparison.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fdchp",
   "language": "python",
   "name": "fdchp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
